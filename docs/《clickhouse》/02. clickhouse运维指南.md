---
title:  clickhouse运维指南
date: 2023-01-29 19:56:14
permalink: /pages/eafa23/
categories:
  - 《clickhouse》
tags:
  - 
---
## 术语
- ClickHouseMergeTree Engine引擎系
### [索引](https://clickhouse.com/docs/zh/guides/improving-query-performance/sparse-primary-indexes/)
主键索引
跳数索引：minmax
排序键

### 

## QA
应该每批提交多少合适


### 表相关
```
# 查询元数据
SELECT
    part_type,
    path,
    formatReadableQuantity(rows) AS rows,
    formatReadableSize(data_uncompressed_bytes) AS data_uncompressed_bytes,
    formatReadableSize(data_compressed_bytes) AS data_compressed_bytes,
    formatReadableSize(primary_key_bytes_in_memory) AS primary_key_bytes_in_memory,
    marks,
    formatReadableSize(bytes_on_disk) AS bytes_on_disk
FROM system.parts
WHERE (table = 'hits_UserID_URL') AND (active = 1)
FORMAT Vertical;
```


### 常见问题
1. Too many parts (30X) 
```
# 检查数据表
check table ww_xxxx_table_local;

# 调整集群配置，效果不佳

mege_tree_max_parts_in_total= 30000

# 继续检查当日表分区的parts

SELECT FQDN() AS node, database, table, 
count() AS count FROM clusterAllReplicas('default_cluster', system.parts) 
WHERE (active = 1) AND (partition = '20230111') 
AND (table = 'ww_xxxx_table_local') 
GROUP BY node, database, table ORDER BY count DESC, node ASC;

# 查看是否有mutation执行
select FQDN() as node, database, table, mutation_id, create_time,
command, is_done, parts_to_do
FROM clusterAllReplicas('default_cluster', system.mutations)
WHERE is_done = 0;

# 执行SQL检查一下文件parts的状态：
select modification_time,FQDN() as node,
partition,name,level,rows 
FROM clusterAllReplicas('default_cluster', system.parts) 
WHERE active = 1 and table = 'ww_xxxx_table_local' 
and partition = '20230111';

# 尝试一下手动merge
optimize table ww_xxxxx_table_local final

# 删除分区
ALTER TABLE ww_xxxx_table_local DROP PARTITION '20230111'

# 查看query_log

# 在clickhouse的参考官方文档中看到，server支持detach/attach操作。这个操作并不会实际删除表数据，只是将其从clickhouse的管理中暂时移除。就如这里前言所描述的，何不做一个技术渣男，尝试“忘记”这个表先，再重新“想起”他，看server是否会唤起这个表的merge处理。
detach table ww_xxxx_table_local on cluster default_cluster;
attach table ww_xxxx_table_local on cluster default_cluster;
```

2. 业务表都read only,https://github.com/ClickHouse/ClickHouse/issues/35863?from_wecom=1

```
# read only 

select table, zookeeper_path,replica_path 
from `system`.replicas where is_readonly; 

# 大概率可能是由于zk集群的同步出现的延迟或者异常OOM,将写入条数调整为2w


```

3. 调优指南
```
（1）集群状态监控：平日在操作alter table drop partition 的时候，其实已经发现需要卡住很久才能执行完成，并且偶现出库数据任务失败，这里其实已经是ZK集群高负载的一个警告了。ZK的监控需要补齐，特别是CPU、内存和负载这些指标，有时CK出问题了，并不一定是CK集群的问题。

（2）及时升级集群：升级集群后可以支持ZK的监控，元数据的同步速度实际快很多；

（3）写入调优：对于写入还是老生常谈的问题，合理限制并发数，加大单次写入数据量，不要写入过少或者过快；

（4）表结构调优：合理分库分表管理。CK表不是Hive表，单个表的字段的数量建议不要太多，上百甚至上千的字段的大宽表，同比少字段的表，在相同数据行数情况下写入实测速度慢了3倍以上，甚至可能写入失败；
```